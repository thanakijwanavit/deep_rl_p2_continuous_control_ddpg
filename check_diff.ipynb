{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ddpg_agents.py', 'r') as file1:\n",
    "    with open('ddpg_agents1.py', 'r') as file2:\n",
    "        same = set(file1).intersection(file2)\n",
    "\n",
    "same.discard('\\n')\n",
    "\n",
    "with open('diff_output.log', 'w') as file_out:\n",
    "    for line in same:\n",
    "        file_out.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('diff_output.log', 'r') as f:\n",
    "    file = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('        where:\\n'\n",
      " '            gamma (float): discount factor\\n'\n",
      " '        self.noise.reset()\\n'\n",
      " '        next_states = torch.from_numpy(np.vstack([e.next_state for e in '\n",
      " 'experiences if e is not None])).float().to(device)\\n'\n",
      " '            critic_target(state, action) -> Q-value\\n'\n",
      " '        θ_target = τ*θ_local + (1 - τ)*θ_target\\n'\n",
      " '        self.theta = theta\\n'\n",
      " '        self.sigma = sigma\\n'\n",
      " '        \"\"\"Update internal state and return it as a noise sample.\"\"\"\\n'\n",
      " '        actor_loss = -self.critic_local(states, actions_pred).mean()\\n'\n",
      " '        critic_loss.backward()\\n'\n",
      " '    def __init__(self, action_size, buffer_size, batch_size, seed):\\n'\n",
      " '        # Compute critic loss\\n'\n",
      " '        Params\\n'\n",
      " '        Q_targets = r + γ * critic_target(next_state, '\n",
      " 'actor_target(next_state))\\n'\n",
      " 'BUFFER_SIZE = int(1e6)  # replay buffer size\\n'\n",
      " '        \"\"\"\\n'\n",
      " '        self.batch_size = batch_size\\n'\n",
      " 'import copy\\n'\n",
      " '            actor_target(state) -> action\\n'\n",
      " '    def reset(self):\\n'\n",
      " '        self.actor_optimizer.step()\\n'\n",
      " 'import random\\n'\n",
      " '        self.seed = random.seed(random_seed)\\n'\n",
      " '        self.memory = deque(maxlen=buffer_size)  # internal memory (deque)\\n'\n",
      " '        actions = torch.from_numpy(np.vstack([e.action for e in experiences '\n",
      " 'if e is not None])).float().to(device)\\n'\n",
      " '    \\n'\n",
      " '        \"\"\"Update policy and value parameters using given batch of '\n",
      " 'experience tuples.\\n'\n",
      " '        self.state = x + dx\\n'\n",
      " '        \"\"\"Return the current size of internal memory.\"\"\"\\n'\n",
      " '    \"\"\"Interacts with and learns from the environment.\"\"\"\\n'\n",
      " '        return (states, actions, rewards, next_states, dones)\\n'\n",
      " '        for target_param, local_param in zip(target_model.parameters(), '\n",
      " 'local_model.parameters()):\\n'\n",
      " '        \"\"\"Initialize parameters and noise process.\"\"\"\\n'\n",
      " '        # Compute Q targets for current states (y_i)\\n'\n",
      " '        experiences = random.sample(self.memory, k=self.batch_size)\\n'\n",
      " '        self.noise = OUNoise(action_size, random_seed)\\n'\n",
      " '        self.memory.add(state, action, reward, next_state, done)\\n'\n",
      " 'import torch\\n'\n",
      " '        actor_loss.backward()\\n'\n",
      " '            action = self.actor_local(state).cpu().data.numpy()\\n'\n",
      " '        Q_expected = self.critic_local(states, actions)\\n'\n",
      " 'class Agent():\\n'\n",
      " '        self.experience = namedtuple(\"Experience\", field_names=[\"state\", '\n",
      " '\"action\", \"reward\", \"next_state\", \"done\"])\\n'\n",
      " '        self.actor_local = Actor(state_size, action_size, '\n",
      " 'random_seed).to(device)\\n'\n",
      " '        Q_targets_next = self.critic_target(next_states, actions_next)\\n'\n",
      " '        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e '\n",
      " 'is not None]).astype(np.uint8)).float().to(device)\\n'\n",
      " '        with torch.no_grad():\\n'\n",
      " '        states, actions, rewards, next_states, dones = experiences\\n'\n",
      " '        actions_pred = self.actor_local(states)\\n'\n",
      " '        states = torch.from_numpy(np.vstack([e.state for e in experiences if '\n",
      " 'e is not None])).float().to(device)\\n'\n",
      " '            target_model: PyTorch model (weights will be copied to)\\n'\n",
      " '        # Critic Network (w/ Target Network)\\n'\n",
      " '        # Minimize the loss\\n'\n",
      " '        critic_loss = F.mse_loss(Q_expected, Q_targets)\\n'\n",
      " 'LR_ACTOR = 1e-3         # learning rate of the actor \\n'\n",
      " '        self.reset()\\n'\n",
      " '        self.soft_update(self.critic_local, self.critic_target, TAU)\\n'\n",
      " '    def act(self, state, add_noise=True):\\n'\n",
      " '        # Get predicted next-state actions and Q values from target models\\n'\n",
      " '        self.critic_optimizer.step()\\n'\n",
      " '    def __init__(self, state_size, action_size, random_seed):\\n'\n",
      " '            random_seed (int): random seed\\n'\n",
      " 'from model import Actor, Critic\\n'\n",
      " '    def sample(self):\\n'\n",
      " '        self.actor_local.train()\\n'\n",
      " '        self.critic_optimizer = optim.Adam(self.critic_local.parameters(), '\n",
      " 'lr=LR_CRITIC, weight_decay=WEIGHT_DECAY)\\n'\n",
      " 'import torch.optim as optim\\n'\n",
      " 'import numpy as np\\n'\n",
      " '        # ---------------------------- update critic '\n",
      " '---------------------------- #\\n'\n",
      " '        # Compute actor loss\\n'\n",
      " '        \\n'\n",
      " '        return np.clip(action, -1, 1)\\n'\n",
      " '        self.actor_optimizer.zero_grad()\\n'\n",
      " '        # Noise process\\n'\n",
      " '        \"\"\"Returns actions for given state as per current policy.\"\"\"\\n'\n",
      " '            target_param.data.copy_(tau*local_param.data + '\n",
      " '(1.0-tau)*target_param.data)\\n'\n",
      " \"            experiences (Tuple[torch.Tensor]): tuple of (s, a, r, s', done) \"\n",
      " 'tuples \\n'\n",
      " 'from collections import namedtuple, deque\\n'\n",
      " '        self.seed = random.seed(seed)\\n'\n",
      " 'class ReplayBuffer:\\n'\n",
      " '        actions_next = self.actor_target(next_states)\\n'\n",
      " '        if add_noise:\\n'\n",
      " '        self.memory.append(e)\\n'\n",
      " 'class OUNoise:\\n'\n",
      " '        \"\"\"Soft update model parameters.\\n'\n",
      " '        self.critic_optimizer.zero_grad()\\n'\n",
      " '            tau (float): interpolation parameter \\n'\n",
      " '        \"\"\"Initialize an Agent object.\\n'\n",
      " '        self.actor_optimizer = optim.Adam(self.actor_local.parameters(), '\n",
      " 'lr=LR_ACTOR)\\n'\n",
      " '    def learn(self, experiences, gamma):\\n'\n",
      " '        e = self.experience(state, action, reward, next_state, done)\\n'\n",
      " '            state_size (int): dimension of each state\\n'\n",
      " '            action_size (int): dimension of each action\\n'\n",
      " '    def soft_update(self, local_model, target_model, tau):\\n'\n",
      " '        # ----------------------- update target networks '\n",
      " '----------------------- #\\n'\n",
      " '        self.state_size = state_size\\n'\n",
      " '        return self.state\\n'\n",
      " '        x = self.state\\n'\n",
      " 'TAU = 1e-3              # for soft update of target parameters\\n'\n",
      " '        self.critic_target = Critic(state_size, action_size, '\n",
      " 'random_seed).to(device)\\n'\n",
      " 'LR_CRITIC = 1e-3        # learning rate of the critic\\n'\n",
      " '    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\\n'\n",
      " '        self.critic_local = Critic(state_size, action_size, '\n",
      " 'random_seed).to(device)\\n'\n",
      " '        self.actor_local.eval()\\n'\n",
      " '        self.soft_update(self.actor_local, self.actor_target, '\n",
      " 'TAU)                     \\n'\n",
      " 'GAMMA = 0.99            # discount factor\\n'\n",
      " '        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, '\n",
      " 'random_seed)\\n'\n",
      " '        self.state = copy.copy(self.mu)\\n'\n",
      " '        # ---------------------------- update actor '\n",
      " '---------------------------- #\\n'\n",
      " '        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\\n'\n",
      " '        \"\"\"Initialize a ReplayBuffer object.\\n'\n",
      " '            batch_size (int): size of each training batch\\n'\n",
      " '        self.actor_target = Actor(state_size, action_size, '\n",
      " 'random_seed).to(device)\\n'\n",
      " '    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\\n'\n",
      " '        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\\n'\n",
      " '        # Save experience / reward\\n'\n",
      " '        self.action_size = action_size\\n'\n",
      " '    def __len__(self):\\n'\n",
      " '    def add(self, state, action, reward, next_state, done):\\n'\n",
      " '        # Actor Network (w/ Target Network)\\n'\n",
      " '        \"\"\"Save experience in replay memory, and use random sample from '\n",
      " 'buffer to learn.\"\"\"\\n'\n",
      " 'device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\\n'\n",
      " '            local_model: PyTorch model (weights will be copied from)\\n'\n",
      " '        self.mu = mu * np.ones(size)\\n'\n",
      " 'import torch.nn.functional as F\\n'\n",
      " '        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\\n'\n",
      " '            buffer_size (int): maximum size of buffer\\n'\n",
      " '        # Replay memory\\n'\n",
      " '        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences '\n",
      " 'if e is not None])).float().to(device)\\n'\n",
      " '        ======\\n'\n",
      " '        \"\"\"Add a new experience to memory.\"\"\"\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
